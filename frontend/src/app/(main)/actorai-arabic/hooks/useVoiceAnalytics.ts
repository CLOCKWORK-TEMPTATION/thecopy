"use client";

import { useState, useCallback, useRef, useEffect } from "react";

// ==================== Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ====================

export interface VoiceMetrics {
  // Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙˆØª (Pitch)
  pitch: {
    value: number; // Hz
    level: "low" | "medium" | "high";
    label: string;
  };
  // Ø´Ø¯Ø© Ø§Ù„ØµÙˆØª (Volume)
  volume: {
    value: number; // dB
    level: "quiet" | "normal" | "loud";
    label: string;
  };
  // Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù… (WPM)
  speechRate: {
    wpm: number;
    level: "slow" | "normal" | "fast";
    warning: string | null;
  };
  // ÙˆØ¶ÙˆØ­ Ø§Ù„Ù…Ø®Ø§Ø±Ø¬ (Articulation)
  articulation: {
    score: number; // 0-100
    level: "poor" | "fair" | "good" | "excellent";
    label: string;
  };
  // Ù†Ù…Ø· Ø§Ù„ØªÙ†ÙØ³
  breathing: {
    isBreathing: boolean;
    breathCount: number;
    warning: string | null;
    lastBreathTime: number;
  };
  // Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ©
  pauses: {
    count: number;
    averageDuration: number;
    isEffective: boolean;
    feedback: string;
  };
}

export interface VoiceAnalyticsState {
  isListening: boolean;
  isSupported: boolean;
  error: string | null;
  metrics: VoiceMetrics;
  waveformData: number[];
  frequencyData: number[];
  history: VoiceMetrics[];
}

const DEFAULT_METRICS: VoiceMetrics = {
  pitch: { value: 0, level: "medium", label: "Ù…ØªÙˆØ³Ø·" },
  volume: { value: 0, level: "normal", label: "Ø¹Ø§Ø¯ÙŠ" },
  speechRate: { wpm: 0, level: "normal", warning: null },
  articulation: { score: 0, level: "fair", label: "Ø¬ÙŠØ¯" },
  breathing: { isBreathing: false, breathCount: 0, warning: null, lastBreathTime: 0 },
  pauses: { count: 0, averageDuration: 0, isEffective: true, feedback: "" },
};

// ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ====================

// ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø¯Ø¯ Ø¥Ù„Ù‰ Ø·Ø¨Ù‚Ø© Ø§Ù„ØµÙˆØª
function analyzePitch(frequency: number): VoiceMetrics["pitch"] {
  if (frequency === 0) {
    return { value: 0, level: "medium", label: "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØµÙˆØª" };
  }

  if (frequency < 150) {
    return { value: frequency, level: "low", label: "Ù…Ù†Ø®ÙØ¶" };
  } else if (frequency < 300) {
    return { value: frequency, level: "medium", label: "Ù…ØªÙˆØ³Ø·" };
  } else {
    return { value: frequency, level: "high", label: "Ù…Ø±ØªÙØ¹" };
  }
}

// ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª
function analyzeVolume(decibels: number): VoiceMetrics["volume"] {
  if (decibels < -50) {
    return { value: decibels, level: "quiet", label: "Ù‡Ø§Ø¯Ø¦" };
  } else if (decibels < -20) {
    return { value: decibels, level: "normal", label: "Ø¹Ø§Ø¯ÙŠ" };
  } else {
    return { value: decibels, level: "loud", label: "Ù…Ø±ØªÙØ¹" };
  }
}

// ØªØ­Ù„ÙŠÙ„ Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù…
function analyzeSpeechRate(wpm: number): VoiceMetrics["speechRate"] {
  if (wpm < 100) {
    return { wpm, level: "slow", warning: "Ø³Ø±Ø¹ØªÙƒ Ø¨Ø·ÙŠØ¦Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹ Ù‚Ù„ÙŠÙ„Ø§Ù‹" };
  } else if (wpm > 180) {
    return { wpm, level: "fast", warning: "âš ï¸ Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ø¨Ø³Ø±Ø¹Ø©! Ø£Ø¨Ø·Ø¦ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ¶ÙˆØ­" };
  } else {
    return { wpm, level: "normal", warning: null };
  }
}

// ØªØ­Ù„ÙŠÙ„ ÙˆØ¶ÙˆØ­ Ø§Ù„Ù…Ø®Ø§Ø±Ø¬
function analyzeArticulation(clarity: number): VoiceMetrics["articulation"] {
  const score = Math.min(100, Math.max(0, clarity));

  if (score < 40) {
    return { score, level: "poor", label: "Ø¶Ø¹ÙŠÙ - Ø±ÙƒØ² Ø¹Ù„Ù‰ ÙˆØ¶ÙˆØ­ Ø§Ù„Ø­Ø±ÙˆÙ" };
  } else if (score < 60) {
    return { score, level: "fair", label: "Ù…Ù‚Ø¨ÙˆÙ„ - ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡" };
  } else if (score < 80) {
    return { score, level: "good", label: "Ø¬ÙŠØ¯ - Ø§Ø³ØªÙ…Ø±!" };
  } else {
    return { score, level: "excellent", label: "Ù…Ù…ØªØ§Ø²! ğŸŒŸ" };
  }
}

// Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† ØªØ±Ø¯Ø¯ Ø§Ù„ØµÙˆØª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (autocorrelation)
function detectPitch(buffer: Float32Array, sampleRate: number): number {
  const SIZE = buffer.length;
  const MAX_SAMPLES = Math.floor(SIZE / 2);
  let bestOffset = -1;
  let bestCorrelation = 0;
  let rms = 0;
  let foundGoodCorrelation = false;
  const correlations = new Array(MAX_SAMPLES);

  // Ø­Ø³Ø§Ø¨ RMS Ù„ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØµÙˆØª
  for (let i = 0; i < SIZE; i++) {
    const val = buffer[i] ?? 0;
    rms += val * val;
  }
  rms = Math.sqrt(rms / SIZE);

  // Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØµÙˆØª Ø¶Ø¹ÙŠÙØ§Ù‹ Ø¬Ø¯Ø§Ù‹ØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø·Ø¨Ù‚Ø© ØµÙˆØª
  if (rms < 0.01) return 0;

  let lastCorrelation = 1;
  for (let offset = 0; offset < MAX_SAMPLES; offset++) {
    let correlation = 0;

    for (let i = 0; i < MAX_SAMPLES; i++) {
      const val1 = buffer[i] ?? 0;
      const val2 = buffer[i + offset] ?? 0;
      correlation += Math.abs(val1 - val2);
    }
    correlation = 1 - correlation / MAX_SAMPLES;
    correlations[offset] = correlation;

    if (correlation > 0.9 && correlation > lastCorrelation) {
      foundGoodCorrelation = true;
      if (correlation > bestCorrelation) {
        bestCorrelation = correlation;
        bestOffset = offset;
      }
    } else if (foundGoodCorrelation) {
      const shift =
        (correlations[bestOffset + 1] - correlations[bestOffset - 1]) /
        correlations[bestOffset];
      return sampleRate / (bestOffset + 8 * shift);
    }
    lastCorrelation = correlation;
  }

  if (bestCorrelation > 0.01) {
    return sampleRate / bestOffset;
  }
  return 0;
}

// ==================== Ø§Ù„Ù€ Hook Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ====================

export function useVoiceAnalytics() {
  const [state, setState] = useState<VoiceAnalyticsState>({
    isListening: false,
    isSupported: typeof window !== "undefined" && !!navigator.mediaDevices,
    error: null,
    metrics: DEFAULT_METRICS,
    waveformData: [],
    frequencyData: [],
    history: [],
  });

  // Ù…Ø±Ø§Ø¬Ø¹ Ù„Ù„ØµÙˆØª
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // Ù…Ø±Ø§Ø¬Ø¹ Ù„Ù„ØªØªØ¨Ø¹
  const silenceStartRef = useRef<number>(0);
  const pauseCountRef = useRef<number>(0);
  const pauseDurationsRef = useRef<number[]>([]);
  const wordCountRef = useRef<number>(0);
  const startTimeRef = useRef<number>(0);
  const lastSpeechTimeRef = useRef<number>(0);
  const breathCountRef = useRef<number>(0);
  const lastBreathTimeRef = useRef<number>(0);

  // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
  const updateMetrics = useCallback(() => {
    if (!analyserRef.current || !audioContextRef.current) return;

    const analyser = analyserRef.current;
    const bufferLength = analyser.fftSize;
    const dataArray = new Float32Array(bufferLength);
    const frequencyArray = new Uint8Array(analyser.frequencyBinCount);

    analyser.getFloatTimeDomainData(dataArray);
    analyser.getByteFrequencyData(frequencyArray);

    // Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª (RMS to dB)
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
      const val = dataArray[i] ?? 0;
      sum += val * val;
    }
    const rms = Math.sqrt(sum / bufferLength);
    const decibels = 20 * Math.log10(rms + 0.0001);

    // Ø§Ù„ÙƒØ´Ù Ø¹Ù† ØªØ±Ø¯Ø¯ Ø§Ù„ØµÙˆØª
    const frequency = detectPitch(dataArray, audioContextRef.current.sampleRate);

    // Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØµÙ…Øª ÙˆØ§Ù„ÙˆÙ‚ÙØ§Øª
    const now = Date.now();
    const isSilent = rms < 0.01;

    if (isSilent) {
      if (silenceStartRef.current === 0) {
        silenceStartRef.current = now;
      }

      const silenceDuration = now - silenceStartRef.current;

      // Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙ†ÙØ³ (ØµÙ…Øª Ù‚ØµÙŠØ± Ø¨ÙŠÙ† 300-800ms)
      if (silenceDuration > 300 && silenceDuration < 800 && lastBreathTimeRef.current + 2000 < now) {
        breathCountRef.current++;
        lastBreathTimeRef.current = now;
      }

      // Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø§Ù„Ø¯Ø±Ø§Ù…ÙŠØ© (Ø£ÙƒØ«Ø± Ù…Ù† 500ms)
      if (silenceDuration > 500 && lastSpeechTimeRef.current > 0) {
        if (pauseDurationsRef.current[pauseDurationsRef.current.length - 1] !== silenceDuration) {
          pauseCountRef.current++;
          pauseDurationsRef.current.push(silenceDuration);
        }
      }
    } else {
      silenceStartRef.current = 0;
      lastSpeechTimeRef.current = now;

      // ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (ØªÙ‚Ø±ÙŠØ¨ÙŠ)
      if (frequency > 80) {
        wordCountRef.current += 0.05; // ØªÙ‚Ø¯ÙŠØ± ØªÙ‚Ø±ÙŠØ¨ÙŠ
      }
    }

    // Ø­Ø³Ø§Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù…
    const elapsedMinutes = (now - startTimeRef.current) / 60000;
    const wpm = elapsedMinutes > 0 ? Math.round(wordCountRef.current / elapsedMinutes) : 0;

    // Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ÙˆÙ‚ÙØ§Øª
    const avgPauseDuration =
      pauseDurationsRef.current.length > 0
        ? pauseDurationsRef.current.reduce((a, b) => a + b, 0) / pauseDurationsRef.current.length
        : 0;

    // ØªØ­Ø¯ÙŠØ¯ ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„ÙˆÙ‚ÙØ§Øª
    const pausesFeedback = avgPauseDuration > 2000
      ? "Ø§Ù„ÙˆÙ‚ÙØ§Øª Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø­Ø§ÙˆÙ„ ØªÙ‚ØµÙŠØ±Ù‡Ø§"
      : avgPauseDuration > 500
      ? "ÙˆÙ‚ÙØ§Øª Ø¯Ø±Ø§Ù…ÙŠØ© Ø¬ÙŠØ¯Ø©! ğŸ‘"
      : "Ø£Ø¶Ù ÙˆÙ‚ÙØ§Øª Ø¯Ø±Ø§Ù…ÙŠØ© Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ù…Ø¹Ù†Ù‰";

    // ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„ØªÙ†ÙØ³
    const breathingWarning =
      now - lastBreathTimeRef.current > 15000 && rms > 0.01
        ? "âš ï¸ ØªØ°ÙƒØ± Ø£Ù† ØªØªÙ†ÙØ³! Ù„Ø§ ØªØ­Ø¨Ø³ Ù†ÙØ³Ùƒ"
        : null;

    // Ø­Ø³Ø§Ø¨ ÙˆØ¶ÙˆØ­ Ø§Ù„Ù…Ø®Ø§Ø±Ø¬ (ØªÙ‚Ø¯ÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ spectral clarity)
    const spectralSum = frequencyArray.reduce((a, b) => a + b, 0);
    const spectralAvg = spectralSum / frequencyArray.length;
    const articulationScore = Math.min(100, (spectralAvg / 128) * 100);

    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
    setState((prev) => ({
      ...prev,
      metrics: {
        pitch: analyzePitch(frequency),
        volume: analyzeVolume(decibels),
        speechRate: analyzeSpeechRate(wpm),
        articulation: analyzeArticulation(articulationScore),
        breathing: {
          isBreathing: isSilent && (now - silenceStartRef.current) > 300,
          breathCount: breathCountRef.current,
          warning: breathingWarning,
          lastBreathTime: lastBreathTimeRef.current,
        },
        pauses: {
          count: pauseCountRef.current,
          averageDuration: avgPauseDuration,
          isEffective: avgPauseDuration > 400 && avgPauseDuration < 2500,
          feedback: pausesFeedback,
        },
      },
      waveformData: Array.from(dataArray.slice(0, 128)),
      frequencyData: Array.from(frequencyArray.slice(0, 64)),
    }));

    // Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„
    animationFrameRef.current = requestAnimationFrame(updateMetrics);
  }, []);

  // Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹
  const startListening = useCallback(async () => {
    try {
      // Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ØªØªØ¨Ø¹Ø§Øª
      silenceStartRef.current = 0;
      pauseCountRef.current = 0;
      pauseDurationsRef.current = [];
      wordCountRef.current = 0;
      startTimeRef.current = Date.now();
      lastSpeechTimeRef.current = 0;
      breathCountRef.current = 0;
      lastBreathTimeRef.current = Date.now();

      // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        }
      });
      streamRef.current = stream;

      // Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙŠØ§Ù‚ Ø§Ù„ØµÙˆØª
      const audioContext = new AudioContext();
      audioContextRef.current = audioContext;

      // Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 2048;
      analyser.smoothingTimeConstant = 0.8;
      analyserRef.current = analyser;

      // ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¨Ø§Ù„Ù…Ø­Ù„Ù„
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);

      setState((prev) => ({
        ...prev,
        isListening: true,
        error: null,
      }));

      // Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„
      updateMetrics();
    } catch (error) {
      setState((prev) => ({
        ...prev,
        error: error instanceof Error ? error.message : "ÙØ´Ù„ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†",
      }));
    }
  }, [updateMetrics]);

  // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹
  const stopListening = useCallback(() => {
    // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // Ø¥ØºÙ„Ø§Ù‚ Ø³ÙŠØ§Ù‚ Ø§Ù„ØµÙˆØª
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø«
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    // Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
    setState((prev) => ({
      ...prev,
      isListening: false,
      history: [...prev.history, prev.metrics],
    }));
  }, []);

  // Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¹ÙŠÙŠÙ†
  const reset = useCallback(() => {
    stopListening();
    setState({
      isListening: false,
      isSupported: typeof window !== "undefined" && !!navigator.mediaDevices,
      error: null,
      metrics: DEFAULT_METRICS,
      waveformData: [],
      frequencyData: [],
      history: [],
    });
  }, [stopListening]);

  // ØªÙ†Ø¸ÙŠÙ Ø¹Ù†Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒÙˆÙ†
  useEffect(() => {
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
    };
  }, []);

  return {
    ...state,
    startListening,
    stopListening,
    reset,
  };
}

export default useVoiceAnalytics;
